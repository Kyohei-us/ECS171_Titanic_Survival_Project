{
 "cells": [
  {
   "source": [
    "# Introduction:\n",
    "\n",
    "    We utilized LogisticRegression, both Naive Bayes, and  SVM models for the algorithm. \n",
    "    For testing, we used K-fold validation to test the accuracy, and grid serach when possible. \n",
    " "
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "# load data and imports\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "train, test= train_test_split(clean_titanic_df,random_state=23, test_size = 0.2)\n",
    "\n",
    "X_train, y_train = train.drop(columns=['Survived']), train['Survived']\n",
    "X_test, y_test = test.drop(columns=['Survived']), test['Survived']\n",
    "\n",
    "clean_titanic_df.head()\n",
    "clean_titanic_df.info()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Sex       891 non-null    int64  \n 3   Age       891 non-null    float64\n 4   SibSp     891 non-null    int64  \n 5   Parch     891 non-null    int64  \n 6   Fare      891 non-null    float64\n 7   Embarked  891 non-null    int64  \ndtypes: float64(2), int64(6)\nmemory usage: 55.8 KB\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "source": [
    "# Testing portion 1:\n",
    "* LogisticRegression \n",
    "\n",
    "* Categorical Naive Bayes : Pclass, Sex, SibSp, Parch, and Embarked are turned into one-hot-encoded attributes. Age and Fare are label encoded(0~n-1)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_log = clean_titanic_df.drop(columns=['Survived'])\n",
    "y_log = clean_titanic_df['Survived']\n",
    "\n",
    "X_log_num = X_log[['Age', 'Fare']]\n",
    "\n",
    "X_log_cat_encoded_Pclass = pd.get_dummies(X_log['Pclass'], prefix='Pclass')\n",
    "print(X_log_cat_encoded_Pclass)\n",
    "X_log_cat_encoded_Sex = pd.get_dummies(X_log['Sex'], prefix='Sex')\n",
    "X_log_cat_encoded_SibSp = pd.get_dummies(X_log['SibSp'], prefix='SibSp')\n",
    "X_log_cat_encoded_Parch = pd.get_dummies(X_log['Parch'], prefix='Parch')\n",
    "X_log_cat_encoded_Embarked = pd.get_dummies(X_log['Embarked'], prefix='Embarked')\n",
    "X_log_cat_encoded = pd.concat([X_log_cat_encoded_Pclass, X_log_cat_encoded_Sex, X_log_cat_encoded_SibSp, X_log_cat_encoded_Parch, X_log_cat_encoded_Embarked], axis=1)\n",
    "\n",
    "X_log =  X_log_cat_encoded.join(X_log_num)\n",
    "\n",
    "X_train_log, X_test_log= train_test_split(X_log,random_state=23, test_size = 0.2)\n",
    "\n",
    "# print(X_train_log)\n",
    "\n",
    "lor = LogisticRegression(penalty= 'none', max_iter= 1000, solver='newton-cg').fit(X_train_log, y_train)\n",
    "print(classification_report(y_test, lor.predict(X_test_log)))\n",
    "print(lor.score(X_test_log, y_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     Pclass_1  Pclass_2  Pclass_3\n",
      "0           0         0         1\n",
      "1           1         0         0\n",
      "2           0         0         1\n",
      "3           1         0         0\n",
      "4           0         0         1\n",
      "..        ...       ...       ...\n",
      "886         0         1         0\n",
      "887         1         0         0\n",
      "888         0         0         1\n",
      "889         1         0         0\n",
      "890         0         0         1\n",
      "\n",
      "[891 rows x 3 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "0.770949720670391\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "# Apply 10-fold cross validation on logistic regression\r\n",
    "\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "i = 0\r\n",
    "average_score = 0\r\n",
    "for train_indices, test_indices in kf.split(X_log):\r\n",
    "    #print(train_indices)\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "    \r\n",
    "    lor = LogisticRegression(penalty= 'none', max_iter= 1000, solver='newton-cg').fit(X_log[start_train:stop_train], y_log[start_train:stop_train])\r\n",
    "    print( classification_report( y_log[start_test:stop_test], lor.predict( X_log[start_test:stop_test] ) ) )\r\n",
    "    print(lor.score( X_log[start_test:stop_test], y_log[start_test:stop_test] ))\r\n",
    "    average_score += lor.score( X_log[start_test:stop_test], y_log[start_test:stop_test] )\r\n",
    "    i+=1\r\n",
    "\r\n",
    "print(\"average score: \", average_score/10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81        51\n",
      "           1       0.77      0.69      0.73        39\n",
      "\n",
      "    accuracy                           0.78        90\n",
      "   macro avg       0.78      0.77      0.77        90\n",
      "weighted avg       0.78      0.78      0.78        90\n",
      "\n",
      "0.7777777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89        69\n",
      "           1       0.60      0.75      0.67        20\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.76      0.80      0.78        89\n",
      "weighted avg       0.85      0.83      0.84        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83        55\n",
      "           1       0.76      0.65      0.70        34\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.78      0.76      0.77        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        44\n",
      "           1       0.86      0.82      0.84        45\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.84      0.84      0.84        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82        52\n",
      "           1       0.73      0.81      0.77        37\n",
      "\n",
      "    accuracy                           0.80        89\n",
      "   macro avg       0.79      0.80      0.79        89\n",
      "weighted avg       0.80      0.80      0.80        89\n",
      "\n",
      "0.797752808988764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84        57\n",
      "           1       0.72      0.66      0.69        32\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.77      0.76      0.76        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83        50\n",
      "           1       0.83      0.64      0.72        39\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.80      0.77      0.78        89\n",
      "weighted avg       0.79      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85        56\n",
      "           1       0.83      0.58      0.68        33\n",
      "\n",
      "    accuracy                           0.80        89\n",
      "   macro avg       0.81      0.75      0.77        89\n",
      "weighted avg       0.80      0.80      0.79        89\n",
      "\n",
      "0.797752808988764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87        59\n",
      "           1       0.74      0.77      0.75        30\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.81      0.82      0.81        89\n",
      "weighted avg       0.83      0.83      0.83        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        56\n",
      "           1       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.83      0.83      0.83        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "average score:  0.8081148564294631\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "# try to find optimal max iteration \r\n",
    "\r\n",
    "max_iterations = [x*100 for x in range(1,10)]\r\n",
    "\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "for i in max_iterations:\r\n",
    "    lor = LogisticRegression(penalty= 'none', max_iter= i, solver='newton-cg').fit(X_train_log, y_train)\r\n",
    "    print(classification_report(y_test, lor.predict(X_test_log)))\r\n",
    "    print(\"Score for max iteration of {}\".format(i))\r\n",
    "    print(lor.score(X_test_log, y_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 100\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 200\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 300\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 400\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 500\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 600\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 700\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 800\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 900\n",
      "0.770949720670391\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "source": [
    "## End of Logistic Regression portion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "# create age categories (DO NO RUN THIS CELL TWICE! IF RAN A SECOND TIME, MOST VALUES WILL BE TURNED INTO 1):\n",
    "age_bins = pd.cut( clean_titanic_df['Age'], 10, retbins = True)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while i < 72:\n",
    "    clean_titanic_df.loc[(clean_titanic_df['Age'] > i) &  (clean_titanic_df['Age'] <= i +8), 'Age'] = j\n",
    "    j += 1\n",
    "    i += 8\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "# create Fare category\r\n",
    "\r\n",
    "fare_bins = pd.qcut(clean_titanic_df['Fare'], 5, retbins = True)\r\n",
    "print(fare_bins)\r\n",
    "\r\n",
    "clean_titanic_df.loc[clean_titanic_df['Fare'] <= 7, 'Fare'] = 0\r\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 7) & (clean_titanic_df['Fare'] <= 10 ), 'Fare'] = 1\r\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 10) & (clean_titanic_df['Fare'] <= 21 ), 'Fare'] = 2\r\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 21) & (clean_titanic_df['Fare'] <= 40 ), 'Fare'] = 3\r\n",
    "clean_titanic_df.loc[clean_titanic_df['Fare'] > 40 , 'Fare'] = 4"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0        (-0.001, 7.854]\n1      (39.688, 512.329]\n2          (7.854, 10.5]\n3      (39.688, 512.329]\n4          (7.854, 10.5]\n             ...        \n886       (10.5, 21.679]\n887     (21.679, 39.688]\n888     (21.679, 39.688]\n889     (21.679, 39.688]\n890      (-0.001, 7.854]\nName: Fare, Length: 891, dtype: category\nCategories (5, interval[float64]): [(-0.001, 7.854] < (7.854, 10.5] < (10.5, 21.679] < (21.679, 39.688] < (39.688, 512.329]], array([  0.    ,   7.8542,  10.5   ,  21.6792,  39.6875, 512.3292]))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "source": [
    "train, test= train_test_split(clean_titanic_df,random_state=23, test_size = 0.2)\r\n",
    "\r\n",
    "X_train, y_train = train.drop(columns=['Survived']), train['Survived']\r\n",
    "X_test, y_test = test.drop(columns=['Survived']), test['Survived']"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 190,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "# Categorical Bayes classifier\r\n",
    "\r\n",
    "from sklearn.naive_bayes import CategoricalNB\r\n",
    "\r\n",
    "categorical_NB2 = CategoricalNB()\r\n",
    "categorical_NB2.fit(X_train, y_train)\r\n",
    "print(classification_report(y_test, categorical_NB2.predict(X_test)))\r\n",
    "print(categorical_NB2.score(X_test, y_test))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.82      0.89      0.85       115\n           1       0.76      0.64      0.69        64\n\n    accuracy                           0.80       179\n   macro avg       0.79      0.76      0.77       179\nweighted avg       0.80      0.80      0.79       179\n\n0.7988826815642458\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "# Apply 10-fold cross validation on Categorical Naive Bayes Classifier\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "from sklearn.model_selection import KFold\r\n",
    "from sklearn.naive_bayes import CategoricalNB\r\n",
    "kf = KFold(n_splits=10)\r\n",
    "\r\n",
    "X = clean_titanic_df.drop(columns=['Survived'])\r\n",
    "y = clean_titanic_df['Survived']\r\n",
    "\r\n",
    "i = 0\r\n",
    "average_score = 0\r\n",
    "for train_indices, test_indices in kf.split(X):\r\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\r\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\r\n",
    "\r\n",
    "\r\n",
    "    categorical_NB2 = CategoricalNB()\r\n",
    "    categorical_NB2.fit(X[start_train:stop_train], y[start_train:stop_train])\r\n",
    "    print(classification_report(y[start_test:stop_test], categorical_NB2.predict(X[start_test:stop_test])))\r\n",
    "    print(categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test]))\r\n",
    "    average_score += categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test])\r\n",
    "    i+=1\r\n",
    "\r\n",
    "print(\"average score: \", average_score/10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.70        51\n",
      "           1       0.61      0.51      0.56        39\n",
      "\n",
      "    accuracy                           0.64        90\n",
      "   macro avg       0.64      0.63      0.63        90\n",
      "weighted avg       0.64      0.64      0.64        90\n",
      "\n",
      "0.6444444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82        69\n",
      "           1       0.43      0.60      0.50        20\n",
      "\n",
      "    accuracy                           0.73        89\n",
      "   macro avg       0.65      0.68      0.66        89\n",
      "weighted avg       0.77      0.73      0.74        89\n",
      "\n",
      "0.7303370786516854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        55\n",
      "           1       0.74      0.68      0.71        34\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.78      0.77      0.77        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83        44\n",
      "           1       0.87      0.76      0.81        45\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.83      0.82      0.82        89\n",
      "weighted avg       0.83      0.82      0.82        89\n",
      "\n",
      "0.8202247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80        52\n",
      "           1       0.72      0.70      0.71        37\n",
      "\n",
      "    accuracy                           0.76        89\n",
      "   macro avg       0.76      0.76      0.76        89\n",
      "weighted avg       0.76      0.76      0.76        89\n",
      "\n",
      "0.7640449438202247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88        57\n",
      "           1       0.85      0.69      0.76        32\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.84      0.81      0.82        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84        50\n",
      "           1       0.82      0.72      0.77        39\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.81      0.80      0.80        89\n",
      "weighted avg       0.81      0.81      0.81        89\n",
      "\n",
      "0.8089887640449438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        56\n",
      "           1       0.68      0.64      0.66        33\n",
      "\n",
      "    accuracy                           0.75        89\n",
      "   macro avg       0.74      0.73      0.73        89\n",
      "weighted avg       0.75      0.75      0.75        89\n",
      "\n",
      "0.7528089887640449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88        59\n",
      "           1       0.78      0.70      0.74        30\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.82      0.80      0.81        89\n",
      "weighted avg       0.83      0.83      0.83        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        56\n",
      "           1       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.80      0.79      0.79        89\n",
      "weighted avg       0.81      0.81      0.81        89\n",
      "\n",
      "0.8089887640449438\n",
      "average score:  0.7790511860174781\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "source": [
    "## End of Categorical Naive Bayes Classifier portioin"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing portion 2: \n",
    "* Gaussian Naive Bayes : Age, Number of sibling, Number of parents, and Fare are used as  numerical attributes\n",
    "\n",
    "* Linear SVM : Same as Gaussian but joined by One-hot-encoded  PClass, Embarked ,and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           Age  SibSp  Parch     Fare\n0    22.000000      1      0   7.2500\n1    38.000000      1      0  71.2833\n2    26.000000      0      0   7.9250\n3    35.000000      1      0  53.1000\n4    35.000000      0      0   8.0500\n..         ...    ...    ...      ...\n886  27.000000      0      0  13.0000\n887  19.000000      0      0  30.0000\n888  29.699118      1      2  23.4500\n889  26.000000      0      0  30.0000\n890  32.000000      0      0   7.7500\n\n[891 rows x 4 columns]\n     Survived\n0           0\n1           1\n2           1\n3           1\n4           0\n..        ...\n886         0\n887         1\n888         0\n889         1\n890         0\n\n[891 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "\n",
    "X_Numerical = clean_titanic_df[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]]\n",
    "print(X_Numerical)\n",
    "y_Numerical = clean_titanic_df[[\"Survived\"]]\n",
    "print(y_Numerical)\n",
    "X_train, X_test, y_train, y_test =train_test_split(X_Numerical,y_Numerical , test_size = 0.20, random_state = 23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.72      0.97      0.83       115\n           1       0.87      0.31      0.46        64\n\n    accuracy                           0.74       179\n   macro avg       0.79      0.64      0.64       179\nweighted avg       0.77      0.74      0.70       179\n\n0.7374301675977654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "Gaussian_NB = GaussianNB()\n",
    "Gaussian_NB.fit(X_train, y_train.values.ravel())\n",
    "print(classification_report(y_test, Gaussian_NB.predict(X_test)))\n",
    "print(Gaussian_NB.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-202-c4f5003c1ec2>, line 46)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-202-c4f5003c1ec2>\"\u001b[0;36m, line \u001b[0;32m46\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# using k-fold (10 fold) cross validation\n",
    "#from sklearn.model_selection import cross_validate\n",
    "#scores = cross_validate(Gaussian_NB, X_Numerical,y_Numerical.values.ravel(),cv = 10, scoring= ('neg_mean_squared_error' , 'accuracy', 'recall_macro','recall_weighted','f1_macro','f1_weighted') )\n",
    "#score_total = 0\n",
    "#MSE_total = 0\n",
    "#for label in scores:\n",
    "   # print(label)\n",
    "#for idx, s in enumerate{scores[\"test_accuracy\"]):\n",
    "#    print(s)\n",
    "#    score_total += s\n",
    "n = 10\n",
    "kf = KFold(n_splits=n)\n",
    "\n",
    "X = X_Numerical\n",
    "y = y_Numerical.values.ravel()\n",
    "\n",
    "Gaussian_NB = GaussianNB()\n",
    "\n",
    "def K_fold_report(X,y,k,clf,report= False):\n",
    "    clf_cpy = copy.deepcopy(clf)\n",
    "    i = 0\n",
    "    average_score = 0\n",
    "    kf = KFold(n_splits=n)\n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "        #print(str(i) + \": \")\n",
    "        start_train, stop_train = train_indices[0], train_indices[-1]+1\n",
    "        start_test, stop_test = test_indices[0], test_indices[-1]+1\n",
    "        clf_cpy = clf\n",
    "        clf_cpy.fit(X[start_train:stop_train], y[start_train:stop_train])\n",
    "        if(report):\n",
    "            print(classification_report(y[start_test:stop_test], clf_cpy.predict(X[start_test:stop_test])))\n",
    "            print(clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test]))\n",
    "        average_score += clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test])\n",
    "        i+=1\n",
    "    print(\"\\n-------->\\n\")\n",
    "    print(\"AVERAGE SCORE: \", average_score/n)\n",
    "\n",
    "K_fold_report(X,y,n,Gaussian_NB)   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "## End of Gaussian Naive Bayes Section"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Using linear kernel for SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n-------->\n\nAVERAGE SCORE:  0.7867290886392011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)\n",
    "categorical = clean_titanic_df[['Pclass', 'Embarked', 'Sex']]\n",
    "X_Categorical = one_hot_encoder.fit_transform(categorical)\n",
    "#print((X_Categorical)[0])\n",
    "#print((X_Categorical)[1])\n",
    "encoded_all = np.concatenate((X_Categorical,X_Numerical.values),axis = 1)\n",
    "X_encoded = (pd.DataFrame(data = encoded_all, columns =['Class1','Class2','Class3',\n",
    "'S','C','Q','Male','Female',\n",
    "\"Age\",\"SibSp\",\"Parch\",\"Fare\"]))\n",
    "y_all = clean_titanic_df[['Survived']].values.ravel()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_encoded)\n",
    "X_encoded = scaler.transform(X_encoded)\n",
    "\n",
    "n = 10\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n-------->\n\nAVERAGE SCORE:  0.7980024968789016\n"
     ]
    }
   ],
   "source": [
    "# linear svm using hinge loss funciton\n",
    "#clf = LinearSVC(dual=False) convergence warning\n",
    "clf = LinearSVC(dual=False)\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n"
   ]
  },
  {
   "source": [
    "### Using GridSearch for penalizing coef C"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       115\n           1       0.68      0.64      0.66        64\n\n    accuracy                           0.77       179\n   macro avg       0.75      0.74      0.74       179\nweighted avg       0.76      0.77      0.76       179\n\n0.7653631284916201\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       115\n           1       0.68      0.64      0.66        64\n\n    accuracy                           0.77       179\n   macro avg       0.75      0.74      0.74       179\nweighted avg       0.76      0.77      0.76       179\n\n0.7653631284916201\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       115\n           1       0.68      0.64      0.66        64\n\n    accuracy                           0.77       179\n   macro avg       0.75      0.74      0.74       179\nweighted avg       0.76      0.77      0.76       179\n\n0.7653631284916201\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       115\n           1       0.68      0.64      0.66        64\n\n    accuracy                           0.77       179\n   macro avg       0.75      0.74      0.74       179\nweighted avg       0.76      0.77      0.76       179\n\n0.7653631284916201\n              precision    recall  f1-score   support\n\n           0       0.81      0.83      0.82       115\n           1       0.68      0.64      0.66        64\n\n    accuracy                           0.77       179\n   macro avg       0.75      0.74      0.74       179\nweighted avg       0.76      0.77      0.76       179\n\n0.7653631284916201\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_encoded,y_all, test_size = 0.20, random_state = 23)\n",
    "coef_C = [a *0.2 for a in range(1,6)]\n",
    "for item in coef_C:\n",
    "    clf = LinearSVC(dual=False, C= item)\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "source": [
    "### Using non-linear kernel for SVM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Polynomial: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8272034956304619\n",
      "\n",
      "RBF: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8677403245942573\n",
      "\n",
      "Sigmoid: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.6655181023720349\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPolynomial: \")\n",
    "clf = SVC(kernel='poly',decision_function_shape = 'ovo')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nRBF: \")\n",
    "clf = SVC(kernel='rbf',gamma = 5 ,decision_function_shape = 'ovo')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nSigmoid: \")\n",
    "clf = SVC(kernel='sigmoid',decision_function_shape = 'ovo')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      " OVR Curves <------------------------> \n",
      "\n",
      "\n",
      "Polynomial: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8272034956304619\n",
      "\n",
      "RBF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86        51\n",
      "           1       0.93      0.64      0.76        39\n",
      "\n",
      "    accuracy                           0.82        90\n",
      "   macro avg       0.85      0.80      0.81        90\n",
      "weighted avg       0.84      0.82      0.82        90\n",
      "\n",
      "0.8222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92        69\n",
      "           1       0.75      0.60      0.67        20\n",
      "\n",
      "    accuracy                           0.87        89\n",
      "   macro avg       0.82      0.77      0.79        89\n",
      "weighted avg       0.86      0.87      0.86        89\n",
      "\n",
      "0.8651685393258427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86        55\n",
      "           1       0.84      0.62      0.71        34\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.82      0.77      0.78        89\n",
      "weighted avg       0.81      0.81      0.80        89\n",
      "\n",
      "0.8089887640449438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.95      0.86        44\n",
      "           1       0.94      0.73      0.83        45\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.86      0.84      0.84        89\n",
      "weighted avg       0.86      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87        52\n",
      "           1       0.90      0.70      0.79        37\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.86      0.82      0.83        89\n",
      "weighted avg       0.85      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88        57\n",
      "           1       0.87      0.62      0.73        32\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.84      0.79      0.80        89\n",
      "weighted avg       0.84      0.83      0.82        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86        50\n",
      "           1       0.96      0.62      0.75        39\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.86      0.80      0.80        89\n",
      "weighted avg       0.85      0.82      0.81        89\n",
      "\n",
      "0.8202247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83        56\n",
      "           1       0.80      0.48      0.60        33\n",
      "\n",
      "    accuracy                           0.76        89\n",
      "   macro avg       0.78      0.71      0.72        89\n",
      "weighted avg       0.77      0.76      0.75        89\n",
      "\n",
      "0.7640449438202247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93        59\n",
      "           1       0.92      0.80      0.86        30\n",
      "\n",
      "    accuracy                           0.91        89\n",
      "   macro avg       0.91      0.88      0.90        89\n",
      "weighted avg       0.91      0.91      0.91        89\n",
      "\n",
      "0.9101123595505618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89        56\n",
      "           1       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.87      0.80      0.82        89\n",
      "weighted avg       0.85      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8350312109862671\n",
      "\n",
      "Sigmoid: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.6655181023720349\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n OVR Curves <------------------------> \\n\")\n",
    "print(\"\\nPolynomial: \")\n",
    "clf = SVC(kernel='poly',decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nRBF: \")\n",
    "clf = SVC(kernel='rbf',gamma = 'auto' ,decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf,True)\n",
    "\n",
    "print(\"\\nSigmoid: \")\n",
    "clf = SVC(kernel='sigmoid',decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n"
   ]
  },
  {
   "source": [
    "# dual form = true: warning\n",
    "# coef C is not working for reqularization\n",
    "# how to search gamma"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## End of SVM Section"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('3.8')"
  },
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}