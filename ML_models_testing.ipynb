{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "    We utilized LogisticRegression, both Naive Bayes, and  SVM models for the algorithm. \n",
    "    For testing, we used K-fold validation to test the accuracy, and grid serach when possible. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 8 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Sex       891 non-null    int64  \n",
      " 3   Age       891 non-null    float64\n",
      " 4   SibSp     891 non-null    int64  \n",
      " 5   Parch     891 non-null    int64  \n",
      " 6   Fare      891 non-null    float64\n",
      " 7   Embarked  891 non-null    int64  \n",
      "dtypes: float64(2), int64(6)\n",
      "memory usage: 55.8 KB\n"
     ]
    }
   ],
   "source": [
    "# load data and imports\n",
    "import pandas as pd\n",
    "import seaborn as sbn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "train, test= train_test_split(clean_titanic_df,random_state=23, test_size = 0.2)\n",
    "\n",
    "X_train, y_train = train.drop(columns=['Survived']), train['Survived']\n",
    "X_test, y_test = test.drop(columns=['Survived']), test['Survived']\n",
    "\n",
    "clean_titanic_df.head()\n",
    "clean_titanic_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_results = {'logistic_regression' : [0, 0], 'categorical_NB': [0,0], 'linear_SVM': [0,0], 'gaussian_NB': [0,0] } # each model is mapped to its average and max result from the k-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing portion 1:\n",
    "* LogisticRegression \n",
    "\n",
    "* Categorical Naive Bayes : Pclass, Sex, SibSp, Parch, and Embarked are turned into one-hot-encoded attributes. Age and Fare are label encoded(0~n-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass_1  Pclass_2  Pclass_3\n",
      "0           0         0         1\n",
      "1           1         0         0\n",
      "2           0         0         1\n",
      "3           1         0         0\n",
      "4           0         0         1\n",
      "..        ...       ...       ...\n",
      "886         0         1         0\n",
      "887         1         0         0\n",
      "888         0         0         1\n",
      "889         1         0         0\n",
      "890         0         0         1\n",
      "\n",
      "[891 rows x 3 columns]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_log = clean_titanic_df.drop(columns=['Survived'])\n",
    "y_log = clean_titanic_df['Survived']\n",
    "\n",
    "X_log_num = X_log[['Age', 'Fare']]\n",
    "\n",
    "X_log_cat_encoded_Pclass = pd.get_dummies(X_log['Pclass'], prefix='Pclass')\n",
    "print(X_log_cat_encoded_Pclass)\n",
    "X_log_cat_encoded_Sex = pd.get_dummies(X_log['Sex'], prefix='Sex')\n",
    "X_log_cat_encoded_SibSp = pd.get_dummies(X_log['SibSp'], prefix='SibSp')\n",
    "X_log_cat_encoded_Parch = pd.get_dummies(X_log['Parch'], prefix='Parch')\n",
    "X_log_cat_encoded_Embarked = pd.get_dummies(X_log['Embarked'], prefix='Embarked')\n",
    "X_log_cat_encoded = pd.concat([X_log_cat_encoded_Pclass, X_log_cat_encoded_Sex, X_log_cat_encoded_SibSp, X_log_cat_encoded_Parch, X_log_cat_encoded_Embarked], axis=1)\n",
    "\n",
    "X_log =  X_log_cat_encoded.join(X_log_num)\n",
    "\n",
    "X_train_log, X_test_log= train_test_split(X_log,random_state=23, test_size = 0.2)\n",
    "\n",
    "# print(X_train_log)\n",
    "\n",
    "lor = LogisticRegression(penalty= 'none', max_iter= 1000, solver='newton-cg').fit(X_train_log, y_train)\n",
    "print(classification_report(y_test, lor.predict(X_test_log)))\n",
    "print(lor.score(X_test_log, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.84      0.81        51\n",
      "           1       0.77      0.69      0.73        39\n",
      "\n",
      "    accuracy                           0.78        90\n",
      "   macro avg       0.78      0.77      0.77        90\n",
      "weighted avg       0.78      0.78      0.78        90\n",
      "\n",
      "0.7777777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89        69\n",
      "           1       0.60      0.75      0.67        20\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.76      0.80      0.78        89\n",
      "weighted avg       0.85      0.83      0.84        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83        55\n",
      "           1       0.76      0.65      0.70        34\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.78      0.76      0.77        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84        44\n",
      "           1       0.86      0.82      0.84        45\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.84      0.84      0.84        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.82        52\n",
      "           1       0.73      0.81      0.77        37\n",
      "\n",
      "    accuracy                           0.80        89\n",
      "   macro avg       0.79      0.80      0.79        89\n",
      "weighted avg       0.80      0.80      0.80        89\n",
      "\n",
      "0.797752808988764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84        57\n",
      "           1       0.72      0.66      0.69        32\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.77      0.76      0.76        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.90      0.83        50\n",
      "           1       0.83      0.64      0.72        39\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.80      0.77      0.78        89\n",
      "weighted avg       0.79      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.93      0.85        56\n",
      "           1       0.83      0.58      0.68        33\n",
      "\n",
      "    accuracy                           0.80        89\n",
      "   macro avg       0.81      0.75      0.77        89\n",
      "weighted avg       0.80      0.80      0.79        89\n",
      "\n",
      "0.797752808988764\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.86      0.87        59\n",
      "           1       0.74      0.77      0.75        30\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.81      0.82      0.81        89\n",
      "weighted avg       0.83      0.83      0.83        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.88        56\n",
      "           1       0.81      0.76      0.78        33\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.83      0.83      0.83        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "average score:  0.8081148564294631\n"
     ]
    }
   ],
   "source": [
    "# Apply 10-fold cross validation on logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "i = 0\n",
    "average_score = 0\n",
    "max_score = 0\n",
    "for train_indices, test_indices in kf.split(X_log):\n",
    "    #print(train_indices)\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\n",
    "    \n",
    "    lor = LogisticRegression(penalty= 'none', max_iter= 1000, solver='newton-cg').fit(X_log[start_train:stop_train], y_log[start_train:stop_train])\n",
    "    print( classification_report( y_log[start_test:stop_test], lor.predict( X_log[start_test:stop_test] ) ) )\n",
    "    print(lor.score( X_log[start_test:stop_test], y_log[start_test:stop_test] ))\n",
    "    if max_score < lor.score(X_log[start_test:stop_test], y_log[start_test:stop_test]):\n",
    "        max_score = lor.score(X_log[start_test:stop_test], y_log[start_test:stop_test])\n",
    "    average_score += lor.score( X_log[start_test:stop_test], y_log[start_test:stop_test] )\n",
    "    i+=1\n",
    "    \n",
    "print(\"average score: \", average_score/10)\n",
    "kfold_results['logistic_regression'][0] = max_score\n",
    "kfold_results['logistic_regression'][1] = average_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 100\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 200\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 300\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 400\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 500\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 600\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 700\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 800\n",
      "0.770949720670391\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.69      0.66      0.67        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.75      0.75       179\n",
      "weighted avg       0.77      0.77      0.77       179\n",
      "\n",
      "Score for max iteration of 900\n",
      "0.770949720670391\n"
     ]
    }
   ],
   "source": [
    "# try to find optimal max iteration \n",
    "\n",
    "max_iterations = [x*100 for x in range(1,10)]\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "for i in max_iterations:\n",
    "    lor = LogisticRegression(penalty= 'none', max_iter= i, solver='newton-cg').fit(X_train_log, y_train)\n",
    "    print(classification_report(y_test, lor.predict(X_test_log)))\n",
    "    print(\"Score for max iteration of {}\".format(i))\n",
    "    print(lor.score(X_test_log, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Logistic Regression portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age categories (DO NO RUN THIS CELL TWICE! IF RAN A SECOND TIME, MOST VALUES WILL BE TURNED INTO 1):\n",
    "age_bins = pd.cut( clean_titanic_df['Age'], 10, retbins = True)\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while i < 72:\n",
    "    clean_titanic_df.loc[(clean_titanic_df['Age'] > i) &  (clean_titanic_df['Age'] <= i +8), 'Age'] = j\n",
    "    j += 1\n",
    "    i += 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0        (-0.001, 7.854]\n",
      "1      (39.688, 512.329]\n",
      "2          (7.854, 10.5]\n",
      "3      (39.688, 512.329]\n",
      "4          (7.854, 10.5]\n",
      "             ...        \n",
      "886       (10.5, 21.679]\n",
      "887     (21.679, 39.688]\n",
      "888     (21.679, 39.688]\n",
      "889     (21.679, 39.688]\n",
      "890      (-0.001, 7.854]\n",
      "Name: Fare, Length: 891, dtype: category\n",
      "Categories (5, interval[float64]): [(-0.001, 7.854] < (7.854, 10.5] < (10.5, 21.679] < (21.679, 39.688] < (39.688, 512.329]], array([  0.    ,   7.8542,  10.5   ,  21.6792,  39.6875, 512.3292]))\n"
     ]
    }
   ],
   "source": [
    "# create Fare category\n",
    "\n",
    "fare_bins = pd.qcut(clean_titanic_df['Fare'], 5, retbins = True)\n",
    "print(fare_bins)\n",
    "\n",
    "clean_titanic_df.loc[clean_titanic_df['Fare'] <= 7, 'Fare'] = 0\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 7) & (clean_titanic_df['Fare'] <= 10 ), 'Fare'] = 1\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 10) & (clean_titanic_df['Fare'] <= 21 ), 'Fare'] = 2\n",
    "clean_titanic_df.loc[(clean_titanic_df['Fare'] > 21) & (clean_titanic_df['Fare'] <= 40 ), 'Fare'] = 3\n",
    "clean_titanic_df.loc[clean_titanic_df['Fare'] > 40 , 'Fare'] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test= train_test_split(clean_titanic_df,random_state=23, test_size = 0.2)\n",
    "\n",
    "X_train, y_train = train.drop(columns=['Survived']), train['Survived']\n",
    "X_test, y_test = test.drop(columns=['Survived']), test['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       115\n",
      "           1       0.76      0.64      0.69        64\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.79      0.76      0.77       179\n",
      "weighted avg       0.80      0.80      0.79       179\n",
      "\n",
      "0.7988826815642458\n"
     ]
    }
   ],
   "source": [
    "# Categorical Bayes classifier\n",
    "\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "categorical_NB2 = CategoricalNB()\n",
    "categorical_NB2.fit(X_train, y_train)\n",
    "print(classification_report(y_test, categorical_NB2.predict(X_test)))\n",
    "print(categorical_NB2.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.75      0.70        51\n",
      "           1       0.61      0.51      0.56        39\n",
      "\n",
      "    accuracy                           0.64        90\n",
      "   macro avg       0.64      0.63      0.63        90\n",
      "weighted avg       0.64      0.64      0.64        90\n",
      "\n",
      "0.6444444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82        69\n",
      "           1       0.43      0.60      0.50        20\n",
      "\n",
      "    accuracy                           0.73        89\n",
      "   macro avg       0.65      0.68      0.66        89\n",
      "weighted avg       0.77      0.73      0.74        89\n",
      "\n",
      "0.7303370786516854\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.85      0.83        55\n",
      "           1       0.74      0.68      0.71        34\n",
      "\n",
      "    accuracy                           0.79        89\n",
      "   macro avg       0.78      0.77      0.77        89\n",
      "weighted avg       0.78      0.79      0.78        89\n",
      "\n",
      "0.7865168539325843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83        44\n",
      "           1       0.87      0.76      0.81        45\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.83      0.82      0.82        89\n",
      "weighted avg       0.83      0.82      0.82        89\n",
      "\n",
      "0.8202247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80        52\n",
      "           1       0.72      0.70      0.71        37\n",
      "\n",
      "    accuracy                           0.76        89\n",
      "   macro avg       0.76      0.76      0.76        89\n",
      "weighted avg       0.76      0.76      0.76        89\n",
      "\n",
      "0.7640449438202247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88        57\n",
      "           1       0.85      0.69      0.76        32\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.84      0.81      0.82        89\n",
      "weighted avg       0.84      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84        50\n",
      "           1       0.82      0.72      0.77        39\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.81      0.80      0.80        89\n",
      "weighted avg       0.81      0.81      0.81        89\n",
      "\n",
      "0.8089887640449438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.81        56\n",
      "           1       0.68      0.64      0.66        33\n",
      "\n",
      "    accuracy                           0.75        89\n",
      "   macro avg       0.74      0.73      0.73        89\n",
      "weighted avg       0.75      0.75      0.75        89\n",
      "\n",
      "0.7528089887640449\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.88        59\n",
      "           1       0.78      0.70      0.74        30\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.82      0.80      0.81        89\n",
      "weighted avg       0.83      0.83      0.83        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.85        56\n",
      "           1       0.77      0.70      0.73        33\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.80      0.79      0.79        89\n",
      "weighted avg       0.81      0.81      0.81        89\n",
      "\n",
      "0.8089887640449438\n",
      "average score:  0.7790511860174781\n"
     ]
    }
   ],
   "source": [
    "# Apply 10-fold cross validation on Categorical Naive Bayes Classifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "X = clean_titanic_df.drop(columns=['Survived'])\n",
    "y = clean_titanic_df['Survived']\n",
    "\n",
    "i = 0\n",
    "average_score = 0\n",
    "max_score\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "    start_train, stop_train = train_indices[0], train_indices[-1]+1\n",
    "    start_test, stop_test = test_indices[0], test_indices[-1]+1\n",
    "\n",
    "\n",
    "    categorical_NB2 = CategoricalNB()\n",
    "    categorical_NB2.fit(X[start_train:stop_train], y[start_train:stop_train])\n",
    "    print(classification_report(y[start_test:stop_test], categorical_NB2.predict(X[start_test:stop_test])))\n",
    "    print(categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test]))\n",
    "    if max_score < categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test]):\n",
    "        max_score = categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test])\n",
    "    average_score += categorical_NB2.score(X[start_test:stop_test], y[start_test:stop_test])\n",
    "    i+=1\n",
    "\n",
    "print(\"average score: \", average_score/10)\n",
    "kfold_results['categorical_NB'][0] = max_score\n",
    "kfold_results['categorical_NB'][1] = average_score/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Categorical Naive Bayes Classifier portion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing portion 2: \n",
    "* Gaussian Naive Bayes : Age, Number of sibling, Number of parents, and Fare are used as  numerical attributes\n",
    "\n",
    "* Linear SVM : Same as Gaussian but joined by One-hot-encoded  PClass, Embarked ,and sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Age  SibSp  Parch     Fare\n",
      "0    22.000000      1      0   7.2500\n",
      "1    38.000000      1      0  71.2833\n",
      "2    26.000000      0      0   7.9250\n",
      "3    35.000000      1      0  53.1000\n",
      "4    35.000000      0      0   8.0500\n",
      "..         ...    ...    ...      ...\n",
      "886  27.000000      0      0  13.0000\n",
      "887  19.000000      0      0  30.0000\n",
      "888  29.699118      1      2  23.4500\n",
      "889  26.000000      0      0  30.0000\n",
      "890  32.000000      0      0   7.7500\n",
      "\n",
      "[891 rows x 4 columns]\n",
      "     Survived\n",
      "0           0\n",
      "1           1\n",
      "2           1\n",
      "3           1\n",
      "4           0\n",
      "..        ...\n",
      "886         0\n",
      "887         1\n",
      "888         0\n",
      "889         1\n",
      "890         0\n",
      "\n",
      "[891 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "\n",
    "X_Numerical = clean_titanic_df[[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]]\n",
    "print(X_Numerical)\n",
    "y_Numerical = clean_titanic_df[[\"Survived\"]]\n",
    "print(y_Numerical)\n",
    "X_train, X_test, y_train, y_test =train_test_split(X_Numerical,y_Numerical , test_size = 0.20, random_state = 23)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.97      0.83       115\n",
      "           1       0.87      0.31      0.46        64\n",
      "\n",
      "    accuracy                           0.74       179\n",
      "   macro avg       0.79      0.64      0.64       179\n",
      "weighted avg       0.77      0.74      0.70       179\n",
      "\n",
      "0.7374301675977654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "Gaussian_NB = GaussianNB()\n",
    "Gaussian_NB.fit(X_train, y_train.values.ravel())\n",
    "print(classification_report(y_test, Gaussian_NB.predict(X_test)))\n",
    "print(Gaussian_NB.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.6824843945068665\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# using k-fold (10 fold) cross validation\n",
    "#from sklearn.model_selection import cross_validate\n",
    "#scores = cross_validate(Gaussian_NB, X_Numerical,y_Numerical.values.ravel(),cv = 10, scoring= ('neg_mean_squared_error' , 'accuracy', 'recall_macro','recall_weighted','f1_macro','f1_weighted') )\n",
    "#score_total = 0\n",
    "#MSE_total = 0\n",
    "#for label in scores:\n",
    "   # print(label)\n",
    "#for idx, s in enumerate{scores[\"test_accuracy\"]):\n",
    "#    print(s)\n",
    "#    score_total += s\n",
    "n = 10\n",
    "kf = KFold(n_splits=n)\n",
    "\n",
    "X = X_Numerical\n",
    "y = y_Numerical.values.ravel()\n",
    "\n",
    "Gaussian_NB = GaussianNB()\n",
    "\n",
    "def K_fold_report(X,y,k,clf,report= False):\n",
    "    clf_cpy = copy.deepcopy(clf)\n",
    "    i = 0\n",
    "    average_score = 0\n",
    "    max_score = 0\n",
    "    kf = KFold(n_splits=n)\n",
    "    for train_indices, test_indices in kf.split(X):\n",
    "        #print(str(i) + \": \")\n",
    "        start_train, stop_train = train_indices[0], train_indices[-1]+1\n",
    "        start_test, stop_test = test_indices[0], test_indices[-1]+1\n",
    "        clf_cpy = clf\n",
    "        clf_cpy.fit(X[start_train:stop_train], y[start_train:stop_train])\n",
    "        if(report):\n",
    "            print(classification_report(y[start_test:stop_test], clf_cpy.predict(X[start_test:stop_test])))\n",
    "            print(clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test]))\n",
    "        average_score += clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test])\n",
    "        if max_score < clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test]):\n",
    "            max_score = clf_cpy.score(X[start_test:stop_test], y[start_test:stop_test])\n",
    "        i+=1\n",
    "    print(\"\\n-------->\\n\")\n",
    "    print(\"AVERAGE SCORE: \", average_score/n)\n",
    "    return [max_score, average_score/n]\n",
    "\n",
    "kfold_results['gaussian_NB'] = K_fold_report(X,y,n,Gaussian_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Gaussian Naive Bayes Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using linear kernel for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.7867290886392011\n",
      "[0.8426966292134831, 0.7867290886392011]\n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.7867290886392011\n",
      "{'logistic_regression': [0.8426966292134831, 0.8081148564294631], 'categorical_NB': [0.8426966292134831, 0.7790511860174781], 'linear_SVM': [0.8426966292134831, 0.7867290886392011], 'gaussian_NB': [0.7415730337078652, 0.6824843945068665]}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "clean_titanic_df = pd.read_csv('preprocessed_titanic.csv')\n",
    "clean_titanic_df = clean_titanic_df.drop(columns=['Name', 'PassengerId', 'Unnamed: 0'])\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse = False)\n",
    "categorical = clean_titanic_df[['Pclass', 'Embarked', 'Sex']]\n",
    "X_Categorical = one_hot_encoder.fit_transform(categorical)\n",
    "#print((X_Categorical)[0])\n",
    "#print((X_Categorical)[1])\n",
    "encoded_all = np.concatenate((X_Categorical,X_Numerical.values),axis = 1)\n",
    "X_encoded = (pd.DataFrame(data = encoded_all, columns =['Class1','Class2','Class3',\n",
    "'S','C','Q','Male','Female',\n",
    "\"Age\",\"SibSp\",\"Parch\",\"Fare\"]))\n",
    "y_all = clean_titanic_df[['Survived']].values.ravel()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_encoded)\n",
    "X_encoded = scaler.transform(X_encoded)\n",
    "\n",
    "n = 10\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "print(K_fold_report(X_encoded,y_all,n,clf))\n",
    "kfold_results['linear_SVM'] = K_fold_report(X_encoded,y_all,n,clf)\n",
    "print(kfold_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.7980024968789016\n"
     ]
    }
   ],
   "source": [
    "# linear svm using hinge loss funciton\n",
    "#clf = LinearSVC(dual=False) convergence warning\n",
    "clf = LinearSVC(dual=False)\n",
    "K_fold_report(X_encoded,y_all,n,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GridSearch for penalizing coef C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.68      0.64      0.66        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.74      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n",
      "0.7653631284916201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.68      0.64      0.66        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.74      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n",
      "0.7653631284916201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.68      0.64      0.66        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.74      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n",
      "0.7653631284916201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.68      0.64      0.66        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.74      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n",
      "0.7653631284916201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82       115\n",
      "           1       0.68      0.64      0.66        64\n",
      "\n",
      "    accuracy                           0.77       179\n",
      "   macro avg       0.75      0.74      0.74       179\n",
      "weighted avg       0.76      0.77      0.76       179\n",
      "\n",
      "0.7653631284916201\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_encoded,y_all, test_size = 0.20, random_state = 23)\n",
    "coef_C = [a *0.2 for a in range(1,6)]\n",
    "for item in coef_C:\n",
    "    clf = LinearSVC(dual=False, C= item)\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using non-linear kernel for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Polynomial: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8272034956304619\n",
      "\n",
      "RBF: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8677403245942573\n",
      "\n",
      "Sigmoid: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.6655181023720349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7303370786516854, 0.6655181023720349]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nPolynomial: \")\n",
    "clf = SVC(kernel='poly',decision_function_shape = 'ovo')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nRBF: \")\n",
    "clf = SVC(kernel='rbf',gamma = 5 ,decision_function_shape = 'ovo')\n",
    "kfold_results['rbf_SVM'] = K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nSigmoid: \")\n",
    "clf = SVC(kernel='sigmoid',decision_function_shape = 'ovo')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " OVR Curves <------------------------> \n",
      "\n",
      "\n",
      "Polynomial: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8272034956304619\n",
      "\n",
      "RBF: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86        51\n",
      "           1       0.93      0.64      0.76        39\n",
      "\n",
      "    accuracy                           0.82        90\n",
      "   macro avg       0.85      0.80      0.81        90\n",
      "weighted avg       0.84      0.82      0.82        90\n",
      "\n",
      "0.8222222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92        69\n",
      "           1       0.75      0.60      0.67        20\n",
      "\n",
      "    accuracy                           0.87        89\n",
      "   macro avg       0.82      0.77      0.79        89\n",
      "weighted avg       0.86      0.87      0.86        89\n",
      "\n",
      "0.8651685393258427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86        55\n",
      "           1       0.84      0.62      0.71        34\n",
      "\n",
      "    accuracy                           0.81        89\n",
      "   macro avg       0.82      0.77      0.78        89\n",
      "weighted avg       0.81      0.81      0.80        89\n",
      "\n",
      "0.8089887640449438\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.95      0.86        44\n",
      "           1       0.94      0.73      0.83        45\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.86      0.84      0.84        89\n",
      "weighted avg       0.86      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.87        52\n",
      "           1       0.90      0.70      0.79        37\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.86      0.82      0.83        89\n",
      "weighted avg       0.85      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88        57\n",
      "           1       0.87      0.62      0.73        32\n",
      "\n",
      "    accuracy                           0.83        89\n",
      "   macro avg       0.84      0.79      0.80        89\n",
      "weighted avg       0.84      0.83      0.82        89\n",
      "\n",
      "0.8314606741573034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.98      0.86        50\n",
      "           1       0.96      0.62      0.75        39\n",
      "\n",
      "    accuracy                           0.82        89\n",
      "   macro avg       0.86      0.80      0.80        89\n",
      "weighted avg       0.85      0.82      0.81        89\n",
      "\n",
      "0.8202247191011236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.93      0.83        56\n",
      "           1       0.80      0.48      0.60        33\n",
      "\n",
      "    accuracy                           0.76        89\n",
      "   macro avg       0.78      0.71      0.72        89\n",
      "weighted avg       0.77      0.76      0.75        89\n",
      "\n",
      "0.7640449438202247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93        59\n",
      "           1       0.92      0.80      0.86        30\n",
      "\n",
      "    accuracy                           0.91        89\n",
      "   macro avg       0.91      0.88      0.90        89\n",
      "weighted avg       0.91      0.91      0.91        89\n",
      "\n",
      "0.9101123595505618\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89        56\n",
      "           1       0.91      0.64      0.75        33\n",
      "\n",
      "    accuracy                           0.84        89\n",
      "   macro avg       0.87      0.80      0.82        89\n",
      "weighted avg       0.85      0.84      0.84        89\n",
      "\n",
      "0.8426966292134831\n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.8350312109862671\n",
      "\n",
      "Sigmoid: \n",
      "\n",
      "-------->\n",
      "\n",
      "AVERAGE SCORE:  0.6655181023720349\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n OVR Curves <------------------------> \\n\")\n",
    "print(\"\\nPolynomial: \")\n",
    "clf = SVC(kernel='poly',decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n",
    "\n",
    "print(\"\\nRBF: \")\n",
    "clf = SVC(kernel='rbf',gamma = 'auto' ,decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf,True)\n",
    "\n",
    "print(\"\\nSigmoid: \")\n",
    "clf = SVC(kernel='sigmoid',decision_function_shape = 'ovr')\n",
    "K_fold_report(X_encoded,y_all,n,clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic_regression': [0.8426966292134831, 0.8081148564294631], 'categorical_NB': [0.8426966292134831, 0.7790511860174781], 'linear_SVM': [0.8426966292134831, 0.7867290886392011], 'gaussian_NB': [0.7415730337078652, 0.6824843945068665], 'rbf_SVM': [0.9325842696629213, 0.8677403245942573]}\n",
      "[0 1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(kfold_results)\n",
    "\n",
    "labels = []\n",
    "maxs = []\n",
    "avgs = []\n",
    "\n",
    "for key, value in kfold_results.items():\n",
    "    \n",
    "    labels.append(key)\n",
    "    maxs.append(value[0])\n",
    "    avgs.append(value[1])\n",
    "    \n",
    "x = np.arange(len(labels))  # the label locations\n",
    "print(x)\n",
    "width = 0.25  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, maxs, width, label='maximum value')\n",
    "rects2 = ax.bar(x + width/2, avgs, width, label='average value')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Maximum and Average Scores from K-fold cross-validation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=10)\n",
    "ax.bar_label(rects2, padding=10)\n",
    "plt.plot(4, 1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dual form = true: warning\n",
    "# coef C is not working for reqularization\n",
    "# how to search gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of SVM Section"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
